{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75adde04",
   "metadata": {},
   "source": [
    "# Python Programming Language for Data Analysis\n",
    "\n",
    "In our earlier workshop we learned how to import third party libraries such as `pandas` and use it to analyze data. In the process, we learned many fundamental aspects of programming such as:\n",
    "\n",
    "    - Variables and Data Types\n",
    "    - Operators\n",
    "    - Functions (User-defined functions, built-in functions, methods and third party functions)\n",
    "    - Indexing and Extracting elements from a sequence\n",
    "    \n",
    "We also learned how to use many core aspects of `pandas` library:\n",
    "\n",
    "    - How to import data in a csv file and get summary statistics for numeric and non-numeric columns\n",
    "    - How to list functions available in pandas modules and review its use by consulting documentations online\n",
    "    - How to filter rows and columns to get a desired subset of data\n",
    "    - How to create new columns with desired values\n",
    "    - How to group data based on one or multiple columns and get group-wise summary statistics\n",
    "    - How to plot data to visualize trends over time\n",
    "    \n",
    "In this workshop, we will now use this knowledge to perform end-to-end data analysis. First, we will begin by answering the questions we have already solved, so that we can practice what we know. Then we will focus on how to use this data such that we can create a simple model that can predict XXX the ridership of the 79th route. For the latter we will utilize the `sklearn` package for machine learning in Python.\n",
    "\n",
    "Let's begin by importing the libraries and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f33900",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filepath_or_buffer=\"cta-ridership-original.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d72a1",
   "metadata": {},
   "source": [
    "Let's answer the following questions first:\n",
    "1. Identify the 10 routes with highest number of ridership in total. Create a bar plot of total ridership of these top 10 routes. To create `bar` plot simple provide argument `bar` to the parameter `kind` of the `plot` method. \n",
    "2. Which route has the highest average ridership? Is it also the most popular route on Saturdays or on Sundays and Holidays? Why is the route so popular?\n",
    "3. Group the data by year to figure out the yearly average trend of ridership over the years. Plot the yearly average of the average monthly total ridership value.\n",
    "4. Now use the above grouped data to plot the average ridership during the weekdays, saturday and sunday/holidays by year.\n",
    "5. Which routes have the highest difference in average ridership between weekdays and Saturdays?\n",
    "6. Which routes have the highest difference in average ridership between weekdays and Sundays/Holidays?\n",
    "7. Which routes have the most consistent average ridership between weekdays, Saturdays and Sundays/Holidays? i.e. are there any route that are not affected by the day of the week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e63a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify the 10 routes with highest number of ridership in total. \n",
    "#    Create a bar plot of total ridership of these top 10 routes. \n",
    "\n",
    "routes_grouped = data[['routename','MonthTotal']].groupby('routename')   # created groupby object\n",
    "monthtotal_byroutes = routes_grouped.sum()                               # get sum for each group in groupby object  \n",
    "top10routes = monthtotal_byroutes.sort_values(by='MonthTotal', ascending=False)[:10] # sort and get the first 10 values\n",
    "\n",
    "top10routes.plot(kind='bar')   # create a bar plot of the series with top 10 values along with the index                                       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e143b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Which route has the highest average ridership? \n",
    "#    Is it also the most popular route on Saturdays or on Sundays and Holidays? \n",
    "#    Why is the route so popular?\n",
    "\n",
    "\n",
    "cols = [ 'Avg_Weekday_Rides','Avg_Saturday_Rides', 'Avg_Sunday-Holiday_Rides'] # create a list of columns to use\n",
    "\n",
    "top10routes_byday = []  # initialize empty list to store results\n",
    "for i in cols:          # iterate over each item of the list\n",
    "    print(i)            # print the item\n",
    "    routes_grouped = data[['routename', i]].groupby('routename') # ceate group byobject with routename and item in the loop\n",
    "    total_byroutes = routes_grouped.sum()                        # get sum of each group\n",
    "    top10routes = total_byroutes.sort_values(by=i, ascending=False)[:10] # sort and extract the first 10 items\n",
    "    top10routes_byday.append(top10routes)                        # append the above result to the empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca047b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in top10routes_byday: # for each item in the list that now has result\n",
    "    i.plot(kind='bar')      # create a bar plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Group the data by year to figure out the yearly average trend of ridership over the years. \n",
    "#    Plot the yearly average of the average monthly total ridership value.\n",
    "\n",
    "\n",
    "data['Month_Beginning'] = pd.to_datetime(data['Month_Beginning'], format='%m/%d/%Y') # convert to datetime object\n",
    "data['Month_Beginning_year'] = data['Month_Beginning'].dt.year                       # create new column with just year info\n",
    "\n",
    "yearly_groups = data.iloc[:,3:8].groupby('Month_Beginning_year').mean()              # select a subset of data and create groupby object and calculate mean of each group\n",
    "yearly_groups.head()                                                                 # show first five lines of the above dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()                   # create plotting objects\n",
    "yearly_groups['MonthTotal'].plot(ax=ax)    # plot a series of a dataframe and attach its axis to the plot object above\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True)) # set x-axis labels to integer\n",
    "ax.set_title('Monthly Total Ridership on CTA busses from 2002 to 2018', # set title for the plot\n",
    "             fontsize = 14) \n",
    "plt.show()                                 # display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1645cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Now use the above grouped data to plot the average ridership during the weekdays, saturday and \n",
    "#    sunday/holidays by year.\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "yearly_groups.iloc[:,:-1].plot(ax=ax)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21772ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Which routes have the highest difference in average ridership between weekdays and Saturdays? \n",
    "# 7. Which routes have the highest difference in average ridership between weekdays and Sundays/Holidays?\n",
    "\n",
    "\n",
    "# create new columns that store differences in ridership among day types\n",
    "data['diff_week_saturday'] = data['Avg_Weekday_Rides'] - data['Avg_Saturday_Rides']    # get difference of two columns of a dataframe\n",
    "data['diff_week_sunday'] = data['Avg_Weekday_Rides'] - data['Avg_Sunday-Holiday_Rides']\n",
    "data['diff_sat_sunday'] = data['Avg_Saturday_Rides'] - data['Avg_Sunday-Holiday_Rides']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns to work with\n",
    "cols = [ 'diff_week_saturday','diff_week_sunday', 'diff_sat_sunday']\n",
    "\n",
    "# create grouped dataframes for each day type and store results in a list\n",
    "total_byroutes = []\n",
    "for i in cols:\n",
    "    print(i)\n",
    "    routes_grouped = data[['routename', i]].groupby('routename')\n",
    "    total_byroutes.append(routes_grouped.sum())\n",
    "    \n",
    "totaldiff_byroutes = pd.concat(total_byroutes, axis=1)    # convert a list of dataframe to single dataframe\n",
    "totaldiff_byroutes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to get the top or bottom N items of a column with its index\n",
    "\n",
    "def get_n_items(diff_col, N=10, sort_asc=True):\n",
    "    if sort_asc:\n",
    "        get_n = diff_col.abs().sort_values()[:N]  # get absolute value and then sort in ascending order and get first N items\n",
    "    else:\n",
    "        get_n = diff_col.abs().sort_values(ascending=False)[:N] # get absolute value and then sort in descending order and get first N items\n",
    "        \n",
    "    return get_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to all columns of the dataframe with difference information and \n",
    "# get top 10 routes\n",
    "top10route_bydiff = totaldiff_byroutes.apply(get_n_items, sort_asc=False)\n",
    "top10route_bydiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to all columns of the dataframe with difference information and \n",
    "# get bottom 10 routes\n",
    "\n",
    "bottom10route_bydiff = totaldiff_byroutes.apply(get_n_items)\n",
    "bottom10route_bydiff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57531e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Which routes have the most consistent average ridership between weekdays, Saturdays and Sundays/Holidays? \n",
    "#    i.e. are there any route that are not affected by the day of the week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfd81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this questions relies on some assumption of what is considered to be \"consistent\"\n",
    "# For out demonstration purpose any absolute difference within 5000 will be considered consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that returns rows from a given col that have values less than N\n",
    "def consistency(col, N=5000):\n",
    "    consistent_rows = col[col.abs()<=N].index\n",
    "    return consistent_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list to store results\n",
    "consistent_routes = [] \n",
    "\n",
    "# no. of columns \n",
    "ncol = totaldiff_byroutes.shape[1] \n",
    "\n",
    "# iterate over no. of columns and columns names\n",
    "for i,j in zip(range(0,ncol) , totaldiff_byroutes.columns):\n",
    "    \n",
    "    # apply the above consistency function on all columns of the dataframe with difference values\n",
    "    consistent_index = totaldiff_byroutes.apply(consistency)\n",
    "    \n",
    "    # take result for one column at a time and append to the initialized list\n",
    "    consistent_routes.append(totaldiff_byroutes[j].loc[consistent_index[i]])\n",
    "\n",
    "# convert a list of series into dataframe    \n",
    "consistent_routes = pd.concat(consistent_routes, axis=1)\n",
    "consistent_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ba562",
   "metadata": {},
   "source": [
    "### Pivot Table\n",
    "\n",
    "The popular Pivot Table feature of Excel can also be replicated in `pandas` using the `pivot_table` method. The  method takes the data to pivot, the column in the data that should be the row of the pivot table, the column(s) that will be the columns of the pivot table, the value to aggregate and the function to use for aggregation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Month_Beginning_month'] = data['Month_Beginning'].dt.month\n",
    "ridership_overtime = pd.pivot_table(data=data.iloc[:,1:], \n",
    "                                    index=['Month_Beginning_year'], \n",
    "                                    columns=['Month_Beginning_month'],\n",
    "                                    values=['MonthTotal'],\n",
    "                                    aggfunc=np.mean)\n",
    "ridership_overtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f70fe9",
   "metadata": {},
   "source": [
    "Additional features are available in the `pivot_table` function such as filling-in the missing values in the data, which is set to NaN by default. Refer to the [official documentation](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) for usage. \n",
    "\n",
    "Now, with the above data, we can create a plot such that we can see the ridership trend of each month over the years. In order to automatically generate the names of the month we can use the `datetime` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt #move up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [dt.date(2022, m, 1).strftime('%B') for m in range(1, 13)] # generate names of the months of a year\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ridership_overtime.plot(kind='line', style=['r*-','bo-'], ax=ax) # added style for some line so that we can get distinct lines\n",
    "plt.legend(months, ncol=3, loc='lower left', title='Month of the year',) # show legend in 3 columns\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f65cd",
   "metadata": {},
   "source": [
    "### Modelling the Data\n",
    "\n",
    "Now that we have explored the data and we can now focus on modelling the data. There are many ways to model the data and the best way really depends on the problem you are trying to solve. Let's say for example, we want to know if it is possible to use the information of the ridership on Weekdays and Saturdays to predict the ridership on Sundays and Holidays.\n",
    "\n",
    "**Note:** Obviously, ridership on Sundays and Holidays are dependent of many other factors besides ridership on Weekdays and Saturday. The modelling is for the purposed of demonstration only. \n",
    "\n",
    "The first part of modelling the data is that the data must be clean and any item in the data must be numeric. This is because machine learning models or statistical models in general do not take data that have textual values or missing values. So such data must be processed and transformed to some reasonable numeric representation before they are used in modelling. \n",
    "\n",
    "In our example, we will be using the two numeric columns and they have no missing data as identified above, we are ready to use this data. We will begin by separating the variables that will be predicted and the variables that will be use to predict. The former is commonly refered to as target (y) and the latter as features (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['Avg_Weekday_Rides','Avg_Saturday_Rides']]\n",
    "y = data['Avg_Sunday-Holiday_Rides']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484be1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde25300",
   "metadata": {},
   "source": [
    "Next, we will split the feature data into training and test set. This is while we use data to train a machine learning model, its performance should be reported on a data that the model has never seen before. This ensure that the model is able to generalize i.e. it has not just learned the training data very well but also some patterns that can help predict future data points. This is very important if we want to use the model in the real world.\n",
    "\n",
    "Usually a 70-30 or 80-20 split is recommended. In our case we will keep 3/4 of the data for training and 1/4 for testing. let's import the `train_test_split` function availablel via the `model_selection` submodule of `sklearn` library. This function takes the feature and target data and give us the desired splits of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdefc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff12f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(X,              # feature data\n",
    "                                       y,              # target data \n",
    "                                       test_size=.25,  # size of the test set\n",
    "                                       random_state=42)# set a random number to get the exact split next time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1012f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6df89",
   "metadata": {},
   "source": [
    "You can see that 1/4 of the data for both features and target are now separated as the test set. \n",
    "\n",
    "Let's now import the `Linear_Regression` model from the `linear_model` submodule of `sklearn`, which we will use to fit our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b0d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b0233",
   "metadata": {},
   "source": [
    "Most of the functions in sklearn can be used in the same way:\n",
    "\n",
    "1. Create an instance of the object in use.\n",
    "2. Use `fit` or `fit_transform` method to fit or transform the data as needed. \n",
    "\n",
    "Unlike the `LabelEncoder` which was used to transform the target, here we want to fit the training data to the model, so we will use the `fit` method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55db929",
   "metadata": {},
   "source": [
    "Now that the data has been fit to the linear model, some model property information will now be available in the `model` object. One of these properties is the `score` method, which return the coefficient of determination of the prediction, also known as R squared. It is the proportion of the variation in the dependent variable that is predictable form the independent variable. We can input the training data to get this score.\n",
    "\n",
    "Other properties of interest are the intercept of the and the coefficients for the linear regression model. We can get these information with the `intercept_` and `coef_` methods respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60703497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the coefficient of determination of the prediction\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4b0cf",
   "metadata": {},
   "source": [
    "While the model R Squared value looks good, this value only measures the fit of training data to the model. How well will this model perform on an unseen test data is the next step of evaluation. Regression models often use the mean squared error metric to evaluate the performance on an unseen data. To calculate this we can use `mean_squared_error` function available throuhg the `metrics` submodule of `sklearn`. The function takes the model prediction on a given data and the actual target value for that dataset. Therefore, we first need to generate predictions from our model on the test set using the `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caeb4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85abde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "mse_lrmodel = mse(y_pred_test, y_test)\n",
    "print(mse_lrmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180feb0",
   "metadata": {},
   "source": [
    "How do we know this is a good enough value?\n",
    "\n",
    "In machine learning, we usually have benchmark model against which we can test the performance. In this case we only have one model, so we can create another model and see which one performs better. \n",
    "\n",
    "We can repeat what we did earlier on another model or we can create a for-loop such that the exact same operation goes through all the models in a list. This latter is obviously better as we do not have to write the same code over and over again. It is also easier from readability perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty list to add sklearn model objects\n",
    "models = []\n",
    "\n",
    "# add the sklearn model objects to the list one by one\n",
    "# while adding the model also give it a name so put the name and model in a tuple\n",
    "models.append(('LR', LinearRegression())) \n",
    "models.append(('DT', DecisionTreeRegressor())) # Ensemble method - collection of many decision trees\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ae28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for name, model in models:\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    mse_score = mse(y_pred_test, y_test)\n",
    "    scores[name] = mse_score\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509c58e",
   "metadata": {},
   "source": [
    "Note that the Decision Tree Regression model has lower mean squared error than the Linear Regression model and therefore, is better.\n",
    "\n",
    "It might also be a good idea to store the fitted model, so that once you can explore more details of the model rather than just the scores. See [here](https://scikit-learn.org/stable/modules/tree.html#tree-regression) to learn more about decision tree model properties and sklearn features available to explore the model details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15c83d",
   "metadata": {},
   "source": [
    "### Adding Categorical Features to our Model\n",
    "\n",
    "The above model does not have the information about the ridership pattern specific to a route. So, adding this information might help predict the Sunday-Holiday ridership behavior better. \n",
    "\n",
    "There are two ways we can continue further: \n",
    "1. Isolate data for each route and repeat the above modelling process on that subset.\n",
    "2. Add the routenames to features and create a model in which case we must convert the text to numeric values.\n",
    "\n",
    "Let's try the first option on one route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03f00c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oneroute = data[data['routename']=='16th/18th']\n",
    "oneroute.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(target_features_data, targetname, models):\n",
    "    \n",
    "    y = target_features_data[targetname]\n",
    "    X = target_features_data.drop(targetname, axis=1)\n",
    "    print(X.shape, y.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=1/4, random_state=42)\n",
    "\n",
    "    scores = {}\n",
    "    for name, model in models:\n",
    "        print(f'fitting model: {model}')\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        mse_score = mse(y_pred_test, y_test)\n",
    "        scores[name] = mse_score\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Avg_Sunday-Holiday_Rides', 'Avg_Weekday_Rides','Avg_Saturday_Rides']\n",
    "scores_oneroute = create_model(oneroute[cols], 'Avg_Sunday-Holiday_Rides', models)\n",
    "scores_oneroute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40193729",
   "metadata": {},
   "source": [
    "Let's now try option 2 where we add the categorical feature routename to our model.\n",
    "\n",
    "What are the ways to convert categorical feature into numeric values? There are many. One popular method is to create a new column for each categorical variable and fill in value 1 for the specified routename column if the row belongs to that routename. This process is also called creating dummy variable.\n",
    "\n",
    "Pandas has an easy way to create dummy variables using the `get_dummies` method. Let's apply that on a toy dataset to see what transformation is taking place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create toy data set with categorica and numeric features\n",
    "test_data = pd.DataFrame( [['apple', 2], ['orange', 5]], columns=['fruits', 'quantity'])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for a categorical column\n",
    "pd.get_dummies(test_data['fruits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df3d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the dummy variables with remaining numeric column of the original data\n",
    "pd.concat([pd.get_dummies(test_data['fruits']), test_data['quantity']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c393d8",
   "metadata": {},
   "source": [
    "The above transformation can also be done via `OneHotEncoder` function available through the `preprocessing` submodule of `sklearn`. This method is easier to use if you have many categorical columns. Here, we will see an example on our toy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create an instance of the one hot encoder object\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# 2. fit the data to the one hot encoder instance\n",
    "ohe.fit_transform(test_data['fruits'].values.reshape(-1,1)) # require the input data to be 2-dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699ac72",
   "metadata": {},
   "source": [
    "The resulting object is a sparse matrix, which cannot be combined to a `dataframe`. Therefore, we must first convert it to a numpy `array` for which there is a `toarray` method provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045cbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. convert the transformed data to an numpy array \n",
    "dummy_fruits = ohe.fit_transform(test_data['fruits'].values.reshape(-1,1)).toarray()\n",
    "dummy_fruits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce7e28",
   "metadata": {},
   "source": [
    "Numpy `array` can be easily converted to a pandas `dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. transform the numpy array to dataframe\n",
    "pd.DataFrame(dummy_fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360abcd",
   "metadata": {},
   "source": [
    "Note that we are missing the column names, which can make it difficult to know which category the column belongs to. This can be easily retrieved using the `categories_` method of the `OneHotEncoder` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64255a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while converting to dataframe add column names as well\n",
    "pd.DataFrame(dummy_fruits, columns=ohe.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8b2cb",
   "metadata": {},
   "source": [
    "Now, let's try this on routenames column of our dataset and see if adding this feature to the model helps predict better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "dummy_routename = ohe.fit_transform(data['routename'].values.reshape(-1,1)).toarray()\n",
    "dummy_routename = pd.DataFrame(dummy_routename, columns=ohe.categories_)\n",
    "dummy_routename.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0463d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Avg_Sunday-Holiday_Rides', 'Avg_Weekday_Rides','Avg_Saturday_Rides']\n",
    "target_features_data = pd.concat([data[cols], dummy_routename], axis=1)\n",
    "scores_routename = create_model(target_features_data, 'Avg_Sunday-Holiday_Rides' , models)\n",
    "scores_routename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80366e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15e970",
   "metadata": {},
   "source": [
    "Clearly, the non-linear modelling fit suits the data and the problem better and adding the routename information helps both linear and non-linear model perform better. \n",
    "\n",
    "## Exercise 1:\n",
    "Create yet another model that has all features of the best model so far and add the additional information from the \"Month_Beginning_year\" column. Does adding this information increase predictability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd48203",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "dummy_year = ohe.fit_transform(data['Month_Beginning_year'].values.reshape(-1,1)).toarray()\n",
    "dummy_year = pd.DataFrame(dummy_year, columns=ohe.categories_)\n",
    "\n",
    "cols = ['Avg_Sunday-Holiday_Rides', 'Avg_Weekday_Rides','Avg_Saturday_Rides']\n",
    "target_features_data = pd.concat([data[cols], dummy_routename, dummy_year], axis=1)\n",
    "scores_routename_year = create_model(target_features_data, 'Avg_Sunday-Holiday_Rides' , models)\n",
    "scores_routename_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c15df",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "Create yet another model that has all features of the best model so far and add the additional information from the \"Month_Beginning_month\" column. Does adding this information increase predictability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_month = OneHotEncoder()\n",
    "dummy_month = ohe_month.fit_transform(data['Month_Beginning_month'].values.reshape(-1,1)).toarray()\n",
    "dummy_month = pd.DataFrame(dummy_month, columns=ohe_month.categories_)\n",
    "\n",
    "cols = ['Avg_Sunday-Holiday_Rides', 'Avg_Weekday_Rides','Avg_Saturday_Rides']\n",
    "target_features_data = pd.concat([data[cols], dummy_routename, dummy_year, dummy_month], axis=1)\n",
    "scores_routename_yymm = create_model(target_features_data, 'Avg_Sunday-Holiday_Rides' , models)\n",
    "scores_routename_yymm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45ddfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
